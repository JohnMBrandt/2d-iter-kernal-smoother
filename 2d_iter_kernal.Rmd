---
title: "2-dimensional iterative kernal smoother"
author: "John Brandt"
date: "3/19/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

This script creates and demonstrates an iterative 2-dimensional kernal smoother with built-in bandwidth optimization using leave one out cross validation. The script also contains functions for visualizing the smoothed results with static graphs as well as GIFs. Finally, there are functions to geospatially join smoothed results to an input geojson and return the smoothed results as a polygonal mean.

## Requirements

The functions require an input dataframe with the following columns:

    * time = "time"
    * latitude = "latitude"
    * longitude = "longitude"
    * value = "value"

In order to make use of the geospatial joining function, the following is required:

    * Point data that can be projected to WGS 84
    * GeoJSON that can be reprojected to WGS 84


## Load dependencies

```{r message=FALSE, warning=FALSE}
library(lubridate)
library(dplyr)
library(tidyr)
library(rgdal)
library(sp)
library(sf)
library(ggplot2)
```

## Create interpolation grid

Later functions require the resulting `grid` dataframe as an input. 

```{r}
y <- seq(1.18,1.47, 0.002)
x <- seq(103.6, 104.1, 0.002)
grid <- expand.grid(x,y)
grid$latlong <- paste(grid$Var2, grid$Var1)
```

## Spatial Join

The `spatial.join` function takes a `grid` dataframe created earlier as an input and joins it to an input geoJSON. To do so, it projects the `grid` dataframe to WGS84 and reprojects the geoJSON to WGS84. Finally, the files are spatially joined using the `over` function in the `sp` package, returning a joined spatial dataframe. 

In order to later plot the kernal smoothed results as a polygonal mean, a grouping column from the resulting `map.join` dataframe must be appended to the `grid` dataframe. 

```{r}
spatial.join <- function(input, geojson) {
  input.sp <- input
  coordinates(input.sp) <- ~Var1+Var2
  input.proj <- CRS("+proj=longlat + datum=WGS84")
  proj4string(input.sp) <- input.proj
  
  map <- readOGR(geojson)
  map <- spTransform(map, input.proj)
  plot(map)
  map.join <- sp::over(input.sp, map)
  return(map.join)
}
```

## calc.smooth

This function contains the math that actually calculates the kernal smoothed values. This is a Guassian kernal with a bandwidth h, calculated as

$$
f(z) = \frac{\sum^n_{i}v_iK_h(z_i,z)}{\sum_i^n K_h(z_,z)}
$$
where

$$
K_h(z_i, z) = exp \Big\{\frac{(x-x_i)^2+(y-y_i)^2}{2h^2}\Big\}
$$

Where $v_i$ is a known value at an observation $(x_i, y_i)$, $(x,y)$ is a point at which the kernal is estimating a value, and $h$ is the bandwidth. The value at $(x,y)$ is estimated by weighing the values at each of $(x_i, y_i)$ by the distance from $(x,y)$ to each of $(x_i, y_i)$ penalized for the bandwidth $h$. As such, the kernal smoother is a form of K-nearest neighbors where $K=n$ and the neighbors are weighted by their euclidean distances.

```{r}
calc.smooth <- function(x, y, data, h) {
    xi <- data$longitude
    yi <- data$latitude
    num.div.den <- (((x - xi)^2) + ((y - yi)^2))/(2*h^2)
    kh <- exp(-(num.div.den))
    calculated <- sum(data$value*kh)/sum(kh)
    return(calculated)
}
```

# Cross-validation for bandwidth

The bandwidth $h$ is chosen via leave-one-out cross-validation (LOOCV). For a single kernal smoother for one set of observations, this is done by simply fitting the kernal smoother to $n-1$ points and estimating $(x_{-i}, y_{-i})$. This $n-1$ fitting and prediction is done for each possible $n-1$ training and $(x_{-i}, y_{-i})$ test sets, and the mean squared error is calculated as $\frac{1}{n} \sum^n_i (Y_i - \hat Y_{-i})^2$.

For an iterative kernal smoother, the bandwidth $h$ is chosen by performing LOOCV at each iterative step and returning the value of $h$ that results in the minimum MSE at each iteration.

```{r}
smooth.cv <- function(h, data) {
  single.date <- function(date, h, data) {
    final.results <- list()
    temp <- data[data$time == date,]
    results=list()
    for (i in c(1:nrow(temp))) {
      try({
        train <- temp[-i,]
        test <- temp[i,]
        estimate <- calc.smooth(test$longitude,
                                test$latitude, train, h)
        results[[i]] <- (estimate-test$value)^2
      }, silent=TRUE)
    }
    mean.results <- mean(unlist(results))
    return(mean.results)
  }
  dates.mse <- list()
  dates <- unique(data$time)
    for (i in c(1:length(dates))) {
      dates.mse[i] <- single.date(dates[i], h, data)
    }
  mean <- mean(unlist(dates.mse), na.rm=T)
  return(mean)
}
```

## Iterative 2-d Kernal Smoother

The function `itersmooth` calculates a kernal smoother for each subset of observations in an input dataset. `itersmooth` works by iteratively applying its helper function`calc.day` to each unique value in a given input column. `itersmooth` returns a dataframe with column names `time`, `lat`, `long`, `value`, `latlong`, and `id`.

```{r}
itersmooth <- function(column, bandwidth, known.data) {
  calc.day <- function(day, h, known.data) {
    data <- known.data
    data <- data[data$time==as_datetime(day),]
    results <- rep(NA, nrow(grid))
    for (i in c(1:nrow(grid))) {
      results[i] <- calc.smooth(grid$Var1[i], grid$Var2[i], data, h)
    }
    return(results)
  }
  results <- list()
  dates <- unique(column)
  for (i in c(1:length(dates))) {
    results[[i]] <- data.frame(
      day <- dates[i],
      lat <- grid$Var2,
      long <- grid$Var1,
      value <- calc.day(dates[i], bandwidth, known.data),
      latlong <- paste(grid$Var2, grid$Var1)
    )
  }
  for (i in c(1:length(results))) {
    results[[i]][6] <- i
    colnames(results[[i]]) <- c("time", "lat", "long", "value", "latlong", "id")
  }
  results <- bind_rows(results)
  return(results)
}
```

## Plotting

Four functions were developed concurrently with the 2-dimensional iterative kernal smoother in order to visualize the results statically and dynamically. The first of these, `create.plots` saves to the output folder a `ggplot2` `PNG` of the imputed values at each observation on the grid. 

```{r}
create.plots <- function(date, imputed.data, known.data, column, lim) {
  subset <- imputed.data[imputed.data[[column]] == date,]
  known.data[[column]] <- as_datetime(known.data[[column]])
  subs.known <- known.data[known.data[[column]] == date,]
  plot1 <- ggplot(data=subset, aes(y=lat, x=long))+
    geom_tile(aes(fill=value), size=3)+
    geom_point(data = subs.known, aes(x=longitude, y=latitude, color=value))+
    theme_void()+
    scale_color_distiller(palette="Spectral", limits=lim, guide = F)+
    scale_fill_distiller(palette = "Spectral", limits=lim, 
                         guide = guide_legend( keyheight = unit(2.5, units = "mm"), 
                         keywidth=unit(10, units = "mm"), label.position = "bottom",
                         title.position = 'top', nrow=1))+
    ggtitle(as.character(as_datetime(date)))+
    theme(legend.position=c(0.77,0.2),
          plot.title = element_text(size= 22, hjust=0.01, color = "#4e4d47", 
                                    margin = margin(b = -0.1, t = 0.4, l = 2, unit = "cm")))
  name <- paste0("./fig_output/", as.character(as_datetime(date)), ".png")
  ggsave(name, plot1, dpi=120, width=8, height=5)
}
```

`map.plot` takes a 2-dimensionally smoothed grid as well as an input geoJSON. It merges the two inputs by an id and generates plots for each unique grouping variable, saving the resulting plots to the specified folder. 

```{r message=FALSE, warning=FALSE}
require(broom)
require(maptools)

map.plot <- function(date, data, column, map, title) {
  map <- gBuffer(map, byid=TRUE, width=0)
  map.df <- invisible(tidy(map, region="SUBZONE_C"))
  subs <- data[data[[column]] == date,]
  #subs$id <- c(0:(nrow(subs)-1))
  #subs$id <- as.character(subs$id)
  map.df <- merge(map.df, subs, by.x="id", by.y="zone")
  mapplot <- ggplot()+
    geom_polygon(data=map.df, aes(x=long, y=lat, group=group, fill=value))+
    theme_void()+
    scale_fill_distiller(palette = "Spectral", limits=c(0,100), 
                         guide = guide_legend( keyheight = unit(2.5, units = "mm"), 
                         keywidth=unit(10, units = "mm"), label.position = "bottom",
                         title.position = 'top', nrow=1), name = title)+
    theme(legend.position=c(0.75,0.2), plot.title = element_text(size= 22, hjust=0.01,
                                       color = "#4e4d47", margin = margin(
                                      b = -0.1, t = 0.4, l = 2, unit = "cm")))+
    ggtitle(as.character(as_datetime(date)))+
    coord_map()
  name <- paste0("./gif/", as.character(as_datetime(date)), ".png")
  ggsave(name, mapplot, dpi=120, width=7, height=4.4)
}
```

# Example - Rainfall data

Precipitation data from 2017 at 50 locations in Singapore is interpolated with the 2-dimensional iterative kernal smoother. 

## Scrape weather data

The below script scrapes the `data.gov.sg` website for 5-minute precipitation data from 50 stations during 2017, a total of 5 million observations. It then aggregates the readings to daily precipitation sums for each station. 

```{r}
library(urltools)
library(jsonlite)
pages <- list()
metadata <- list()

rain_url <- "https://api.data.gov.sg/v1/environment/rainfall?date="

# If data is stored in date-time format
#dates <- seq(ymd_hms('2018-01-01 00:00:00'), ymd_hms('2018-01-03 23:00:00'), by="4 hours")

# If data is stored in date format
dates <- seq(ymd('2017-10-01'), ymd('2017-12-31'), by="1 day")

dates <- as.character(dates)
for (i in seq_along(dates)) {
  dates[i] <- gsub(" ", "T", dates[i])
}
dates_enc <- toupper(url_encode(dates))

# This function executes the scrape
for(i in c(1:length(dates_enc))){
  try({
    mydata <- fromJSON(paste0(rain_url, dates_enc[i]))
    pages[[i]] <- mydata$items
    metadata[[i]] <- mydata$metadata
  })
}

rain.meta <- metadata[[2]]$stations

results <- rbind(pages)
weather.all <- unlist(rbind(results[sapply(results, length)>0]), recursive=FALSE)

weather.times <- unlist(weather.all[seq_along(weather.all) %% 2 > 0])
weather.times <- data.frame(
  time = ymd_hms(gsub("T", " ", weather.times)),
  obs = seq(1,length(weather.times)))

weather.readings <- do.call("rbind",  weather.all[seq_along(weather.all) %% 2 == 0])

for (i in c(1:length(weather.readings))) {
  weather.readings[[i]][3] <- i
}

weather.readings <- do.call("rbind", weather.readings)
weather.readings <- merge(weather.readings, weather.times, by.x="V3", by.y="obs")[,-1]
```

```{r}
rain <- merge(weather.readings, rain.meta, by.x="station_id", by.y="id")
rain$time <- as_date(rain$time)
rain$location <- paste(rain$location$latitude, rain$location$longitude)
rain <- rain %>%
  group_by(time, location) %>%
  summarise(rain = sum(value)) %>%
  separate(location, into=c("latitude", "longitude"), sep=" ")

colnames(rain) <- c("time", "latitude", "longitude", "value")
rain$latitude <- as.numeric(rain$latitude)
rain$longitude <- as.numeric(rain$longitude)
```

## Alternatively - read in CSV

The above web-scraper allows the script to be rerun and to incorporate new observations with ease. However, the 2-D iterative kernal smoother can just as well take a standard `.csv`

```{r, eval=FALSE}
rain <- read.csv("rainfall_2017.csv")
rain <- flatten(rain)
rain <- rain[,c(3,4,7,8)]
colnames(rain) <- c("time", "value", "latitude", "longitude")
rain$time <- as_date(rain$time)
```

## CV for bandwidth selection

The `smooth.cv` function is applied to a range of test bandwidths with `lapply`, the results of which are plotted. 0.025 is selected as the optimum bandwidth because it results in the minimum MSE. 

```{r}
test.bw <- seq(0.01, 0.1, 0.005)
mse <- lapply(test.bw, smooth.cv, data = rain)
plot(test.bw, mse)
bandwidth <- test.bw[which.min(mse)]
```

## Create smoothed dataframe

```{r}
rainsmooth <- itersmooth(rain$time, 0.025, rain)
```

## Join the grid to geojson

```{r}
map.join <- spatial.join(grid, "Singapore_subdistricts.geojson")
grid$region <- map.join$REGION_N
grid$zone <- map.join$SUBZONE_C
```

## Subset imputed results to the grid

```{r message=FALSE, warning=FALSE}
rain.singapore <- left_join(rainsmooth, grid[,3:5]) %>%
  na.omit()
```

## Daily rainfall imputed for each zone in Singapore

The `map.plots` function requires a column by which to join the data to the geoJSON. Daily rainfall at each interpolated point is aggregated to calculate a mean within each zone. 

```{r}
rain.zone <- rain.singapore %>%
  na.omit() %>%
  group_by(time, region, zone) %>%
  summarise(value = mean(value))
```

## Plots of imputed results

```{r message=FALSE, warning=FALSE}
invisible(lapply(unique(rain.singapore$time),
       create.plots,
       imputed.data = rain.singapore,
       known.data = rain,
       column = "time",
       lim = c(0,100)))
```

## Make gif

```{r message=FALSE, warning=FALSE}
require(purrr)
require(magick)

list.files(path = "./fig_output", pattern = "*.png", full.names = T) %>% 
  map(image_read) %>% # reads each path file
  image_join() %>% # joins image
    image_animate(fps=10) %>%
  image_write("rainfall.gif") # write to current dir
```

![](rainfall.gif)

## Gif of zone-aggregated rainfall

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
require(rgeos)
map <- readOGR("Singapore_subdistricts.geojson")

invisible(lapply(unique(rain.zone$time), map.plot,
                data = rain.zone,
                column = "time",
                map = map,
                title = "Precipitation (mm)"))

list.files(path = "./gif", pattern = "*.png", full.names = T) %>% 
  map(image_read) %>% # reads each path file
  image_join() %>% # joins image
  image_animate(fps=10) %>% # animates, can opt for number of loops
  image_write("rainfall_zone.gif") # write to current dir
```

![](rainfall_zone.gif)
